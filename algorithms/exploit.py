import time

import cv2

from algorithms.algo_utils import main_observation, goal_observation
from algorithms.env_wrappers import reset_with_info
from utils.utils import log


def run_policy_loop(agent, env, max_num_episodes, fps=7, max_num_frames=None, deterministic=False):
    """Execute the policy and render onto the screen, using the standard agent interface."""
    agent.initialize()

    episode_rewards = []
    num_frames = 0

    def max_frames_reached(frames):
        return max_num_frames is not None and frames > max_num_frames

    for _ in range(max_num_episodes):
        env_obs, info = reset_with_info(env)
        done = False
        obs, goal_obs = main_observation(env_obs), goal_observation(env_obs)
        if goal_obs is not None:
            goal_obs_rgb = cv2.cvtColor(goal_obs, cv2.COLOR_BGR2RGB)
            cv2.imshow('goal', cv2.resize(goal_obs_rgb, (500, 500)))
            cv2.waitKey(500)
        episode_reward = 0

        while not done:
            automap = env.unwrapped.get_automap_buffer()  # (600, 800, 3)
            from PIL import Image, ImageChops

            def trim(im):
                bg = Image.new(im.mode, im.size, im.getpixel((0, 0)))
                diff = ImageChops.difference(im, bg)
                diff = ImageChops.add(diff, diff, 2.0, -100)
                bbox = diff.getbbox()
                if bbox:
                    return im.crop(bbox)

            automap = trim(automap)

            start = time.time()
            env.render()
            if fps < 1000:
                time.sleep(1.0 / fps)
            action = agent.best_action([obs], goals=[goal_obs], deterministic=deterministic)
            env_obs, rew, done, _ = env.step(action)
            obs, goal_obs = main_observation(env_obs), goal_observation(env_obs)
            episode_reward += rew
            log.info('Actual fps: %.1f', 1.0 / (time.time() - start))

            num_frames += 1
            if max_frames_reached(num_frames):
                break

        env.render()
        time.sleep(0.2)

        episode_rewards.append(episode_reward)
        last_episodes = episode_rewards[-100:]
        avg_reward = sum(last_episodes) / len(last_episodes)
        log.info(
            'Episode reward: %f, avg reward for %d episodes: %f', episode_reward, len(last_episodes), avg_reward,
        )

        if max_frames_reached(num_frames):
            break

    agent.finalize()
    env.close()
    cv2.destroyAllWindows()
    return 0
